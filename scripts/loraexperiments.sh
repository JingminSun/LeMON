GPU=0

user=aaa
pre_train_types=( advection conservation_sinflux diff_bistablereact_1D fplanck heat inviscid_conservation_cubicflux Klein_Gordon)
pre_train_typevec=[advection,conservation_sinflux,diff_bistablereact_1D,fplanck,heat,inviscid_conservation_cubicflux,Klein_Gordon]
ft_types=(burgers inviscid_burgers conservation_cubicflux diff_linearreact_1D diff_squarelogisticreact_1D  cahnhilliard_1D Sine_Gordon inviscid_conservation_sinflux kdv diff_logisreact_1D  porous_medium wave)
ft_typevec=[burgers,inviscid_burgers,conservation_cubicflux,diff_linearreact_1D,diff_squarelogisticreact_1D,cahnhilliard_1D,Sine_Gordon,inviscid_conservation_sinflux,kdv,diff_logisreact_1D,porous_medium,wave]
CUDA_VISIBLE_DEVICES=$GPU python src/main.py  exp_id=5stepmamlfull_size1000 wandb.id=5stepmamlfull_size1000 train_size_get=500 batch_size_task=5 meta_step=5 data.train_types=$pre_train_typevec data.eval_types=$pre_train_typevec model.meta.gd_eta=0 model.name=prose
CUDA_VISIBLE_DEVICES=$GPU python src/main.py  exp_id=pretrain_full_size_1000 wandb.id=pretrain_full_size_1000 train_size_get=1000 batch_size=150 data.train_types=$pre_train_typevec data.eval_types=$pre_train_typevec meta=0 model.name=prose


CUDA_VISIBLE_DEVICES=$GPU python src/main.py  exp_id=5stepmaml_reversefull_size1000 wandb.id=5stepmamlfull_size1000 train_size_get=500 batch_size_task=5 meta_step=5 data.train_types=$ft_typevec data.eval_types=$ft_typevec model.meta.gd_eta=0 model.name=prose
CUDA_VISIBLE_DEVICES=$GPU python src/main.py  exp_id=pretrain_reversefull_size1000 wandb.id=pretrain_full_size_1000 train_size_get=1000 batch_size=150 data.train_types=$ft_typevec data.eval_types=$ft_typevec meta=0 model.name=prose
for type in "${ft_types[@]}"
do
  echo "Finetuning $type"
    pre_train=5stepmamlfull_size1000
#  NUMEXPR_MAX_THREADS=12 CUDA_VISIBLE_DEVICES=$GPU python src/main.py batch_size_eval=512 exp_name=fftune exp_id=lora_64_10_MAML_1000_${type} wandb.id=lora_64_10_MAML_1000_${type} save_periodic=-1 reload_model=checkpoint/${user/dumped/LeMON_PROSE/${pre_train}/checkpoint.pth finetune=1 finetune_name=lora lora_r=64 lora_alpha=5 train_size_get=10 eval_size_get=40000 zero_shot_only=1  batch_size=10  n_steps_per_epoch=1  max_epoch=100 log_periodic=1 data.train_types=$type data.eval_types=$type model.name=prose meta=0
  NUMEXPR_MAX_THREADS=12 CUDA_VISIBLE_DEVICES=$GPU python src/main.py batch_size_eval=512 exp_name=fftune exp_id=lora_64_20_MAML_1000_${type} wandb.id=lora_64_20_MAML_1000_${type} save_periodic=-1 reload_model=checkpoint/${user}/dumped/LeMON_PROSE/${pre_train}/checkpoint.pth finetune=1 finetune_name=lora lora_r=64 lora_alpha=5 train_size_get=20 eval_size_get=40000 zero_shot_only=1  batch_size=20  n_steps_per_epoch=1  max_epoch=100 log_periodic=1 data.train_types=$type data.eval_types=$type model.name=prose meta=0
  NUMEXPR_MAX_THREADS=12 CUDA_VISIBLE_DEVICES=$GPU python src/main.py batch_size_eval=512 exp_name=fftune exp_id=lora_64_50_MAML_1000_${type} wandb.id=lora_64_50_MAML_1000_${type} save_periodic=-1 reload_model=checkpoint/${user}/dumped/LeMON_PROSE/${pre_train}/checkpoint.pth finetune=1 finetune_name=lora lora_r=64 lora_alpha=5 train_size_get=50 eval_size_get=40000 zero_shot_only=1  batch_size=50  n_steps_per_epoch=1  max_epoch=100 log_periodic=1 data.train_types=$type data.eval_types=$type model.name=prose meta=0
  NUMEXPR_MAX_THREADS=12 CUDA_VISIBLE_DEVICES=$GPU python src/main.py batch_size_eval=512 exp_name=fftune exp_id=lora_64_100_MAML_1000_${type} wandb.id=lora_64_100_MAML_1000_${type} save_periodic=-1 reload_model=checkpoint/${user}/dumped/LeMON_PROSE/${pre_train}/checkpoint.pth finetune=1 finetune_name=lora lora_r=64 lora_alpha=5 train_size_get=100 eval_size_get=40000 zero_shot_only=1  batch_size=100  n_steps_per_epoch=1  max_epoch=100 log_periodic=1 data.train_types=$type data.eval_types=$type model.name=prose meta=0

#  NUMEXPR_MAX_THREADS=12 CUDA_VISIBLE_DEVICES=$GPU python src/main.py batch_size_eval=256 exp_name=evaluate eval_only=1 exp_id=eval_after_lora_64_100_TL_1000_${type} wandb.id=eval_after_lora_64_100_TL_1000_${type} zero_shot_only=1 data.eval_types=$train_type  eval_from_exp=checkpoint/${user/dumped/fftune/lora_64_100_TL_1000_${type}/checkpoint.pth model=prose meta=0

done


for type in "${types[@]}"
do
  echo "Finetuning $type"
  pre_train=pretrain_full_size_1000
  NUMEXPR_MAX_THREADS=12 CUDA_VISIBLE_DEVICES=$GPU python src/main.py batch_size_eval=512 exp_name=fftune exp_id=lora20_TL_1000_${type} wandb.id=lora20_TL_1000_${type} save_periodic=-1 reload_model=checkpoint/${user}/dumped/LeMON_PROSE/${pre_train}/checkpoint.pth finetune=1 finetune_name=lora lora_r=64 lora_alpha=5 train_size_get=20 eval_size_get=40000 zero_shot_only=1  batch_size=20  n_steps_per_epoch=1  max_epoch=100 log_periodic=1 data.train_types=$type data.eval_types=$type model.name=prose meta=0
  NUMEXPR_MAX_THREADS=12 CUDA_VISIBLE_DEVICES=$GPU python src/main.py batch_size_eval=512 exp_name=fftune exp_id=lora50_TL_1000_${type} wandb.id=lora50_TL_1000_${type} save_periodic=-1 reload_model=checkpoint/${user}/dumped/LeMON_PROSE/${pre_train}/checkpoint.pth finetune=1 finetune_name=lora lora_r=64 lora_alpha=5 train_size_get=50 eval_size_get=40000 zero_shot_only=1  batch_size=50  n_steps_per_epoch=1  max_epoch=100 log_periodic=1 data.train_types=$type data.eval_types=$type model.name=prose meta=0
  NUMEXPR_MAX_THREADS=12 CUDA_VISIBLE_DEVICES=$GPU python src/main.py batch_size_eval=512 exp_name=fftune exp_id=lora100_TL_1000_${type} wandb.id=lora100_TL_1000_${type} save_periodic=-1 reload_model=checkpoint/${user}/dumped/LeMON_PROSE/${pre_train}/checkpoint.pth finetune=1 finetune_name=lora lora_r=64 lora_alpha=5 train_size_get=100 eval_size_get=40000 zero_shot_only=1  batch_size=100  n_steps_per_epoch=1  max_epoch=100 log_periodic=1 data.train_types=$type data.eval_types=$type model.name=prose meta=0

  NUMEXPR_MAX_THREADS=12 CUDA_VISIBLE_DEVICES=$GPU python src/main.py batch_size_eval=512 exp_name=fftune exp_id=ft20_TL_1000_${type} wandb.id=ft20_TL_1000_${type} save_periodic=-1 reload_model=checkpoint/${user}/dumped/LeMON_PROSE/${pre_train}/checkpoint.pth finetune=1 finetune_name=reg train_size_get=20 eval_size_get=40000 zero_shot_only=1  batch_size=20  n_steps_per_epoch=1  max_epoch=100 log_periodic=1 data.train_types=$type data.eval_types=$type model.name=prose meta=0
  NUMEXPR_MAX_THREADS=12 CUDA_VISIBLE_DEVICES=$GPU python src/main.py batch_size_eval=512 exp_name=fftune exp_id=ft50_TL_1000_${type} wandb.id=ft50_TL_1000_${type} save_periodic=-1 reload_model=checkpoint/${user}/dumped/LeMON_PROSE/${pre_train}/checkpoint.pth finetune=1 finetune_name=reg train_size_get=50 eval_size_get=40000 zero_shot_only=1  batch_size=50  n_steps_per_epoch=1  max_epoch=100 log_periodic=1 data.train_types=$type data.eval_types=$type model.name=prose meta=0
  NUMEXPR_MAX_THREADS=12 CUDA_VISIBLE_DEVICES=$GPU python src/main.py batch_size_eval=512 exp_name=fftune exp_id=ft100_TL_1000_${type} wandb.id=ft100_TL_1000_${type} save_periodic=-1 reload_model=checkpoint/${user}/dumped/LeMON_PROSE/${pre_train}/checkpoint.pth finetune=1 finetune_name=reg train_size_get=100 eval_size_get=40000 zero_shot_only=1  batch_size=100  n_steps_per_epoch=1  max_epoch=100 log_periodic=1 data.train_types=$type data.eval_types=$type model.name=prose meta=0

 pre_train=5stepmamlfull_size1000
#  NUMEXPR_MAX_THREADS=12 CUDA_VISIBLE_DEVICES=$GPU python src/main.py batch_size_eval=512 exp_name=fftune exp_id=lora10_MAML_1000_${type} wandb.id=lora10_MAML_1000_${type} save_periodic=-1 reload_model=checkpoint/${user/dumped/LeMON_PROSE/${pre_train}/checkpoint.pth finetune=1 finetune_name=lora lora_r=64 lora_alpha=5 train_size_get=10 eval_size_get=40000 zero_shot_only=1  batch_size=10  n_steps_per_epoch=1  max_epoch=100 log_periodic=1 data.train_types=$type data.eval_types=$type model.name=prose meta=0
  NUMEXPR_MAX_THREADS=12 CUDA_VISIBLE_DEVICES=$GPU python src/main.py batch_size_eval=512 exp_name=fftune exp_id=lora20_MAML_1000_${type} wandb.id=lora20_MAML_1000_${type} save_periodic=-1 reload_model=checkpoint/${user}/dumped/LeMON_PROSE/${pre_train}/checkpoint.pth finetune=1 finetune_name=lora lora_r=64 lora_alpha=5 train_size_get=20 eval_size_get=40000 zero_shot_only=1  batch_size=20  n_steps_per_epoch=1  max_epoch=100 log_periodic=1 data.train_types=$type data.eval_types=$type model.name=prose meta=0
  NUMEXPR_MAX_THREADS=12 CUDA_VISIBLE_DEVICES=$GPU python src/main.py batch_size_eval=512 exp_name=fftune exp_id=lora50_MAML_1000_${type} wandb.id=lora50_MAML_1000_${type} save_periodic=-1 reload_model=checkpoint/${user}/dumped/LeMON_PROSE/${pre_train}/checkpoint.pth finetune=1 finetune_name=lora lora_r=64 lora_alpha=5 train_size_get=50 eval_size_get=40000 zero_shot_only=1  batch_size=50  n_steps_per_epoch=1  max_epoch=100 log_periodic=1 data.train_types=$type data.eval_types=$type model.name=prose meta=0
  NUMEXPR_MAX_THREADS=12 CUDA_VISIBLE_DEVICES=$GPU python src/main.py batch_size_eval=512 exp_name=fftune exp_id=lora100_MAML_1000_${type} wandb.id=lora100_MAML_1000_${type} save_periodic=-1 reload_model=checkpoint/${user}/dumped/LeMON_PROSE/${pre_train}/checkpoint.pth finetune=1 finetune_name=lora lora_r=64 lora_alpha=5 train_size_get=100 eval_size_get=40000 zero_shot_only=1  batch_size=100  n_steps_per_epoch=1  max_epoch=100 log_periodic=1 data.train_types=$type data.eval_types=$type model.name=prose meta=0


   NUMEXPR_MAX_THREADS=12 CUDA_VISIBLE_DEVICES=$GPU python src/main.py batch_size_eval=512 exp_name=fftune exp_id=ft20_MAML_1000_${type} wandb.id=ft20_MAML_1000_${type} save_periodic=-1 reload_model=checkpoint/${user}/dumped/LeMON_PROSE/${pre_train}/checkpoint.pth finetune=1 finetune_name=reg train_size_get=20 eval_size_get=40000 zero_shot_only=1  batch_size=20  n_steps_per_epoch=1  max_epoch=100 log_periodic=1 data.train_types=$type data.eval_types=$type model.name=prose meta=0
  NUMEXPR_MAX_THREADS=12 CUDA_VISIBLE_DEVICES=$GPU python src/main.py batch_size_eval=512 exp_name=fftune exp_id=ft50_MAML_1000_${type} wandb.id=ft50_MAML_1000_${type} save_periodic=-1 reload_model=checkpoint/${user}/dumped/LeMON_PROSE/${pre_train}/checkpoint.pth finetune=1 finetune_name=reg train_size_get=50 eval_size_get=40000 zero_shot_only=1  batch_size=50  n_steps_per_epoch=1  max_epoch=100 log_periodic=1 data.train_types=$type data.eval_types=$type model.name=prose meta=0
  NUMEXPR_MAX_THREADS=12 CUDA_VISIBLE_DEVICES=$GPU python src/main.py batch_size_eval=512 exp_name=fftune exp_id=ft100_MAML_1000_${type} wandb.id=ft100_MAML_1000_${type} save_periodic=-1 reload_model=checkpoint/${user}/dumped/LeMON_PROSE/${pre_train}/checkpoint.pth finetune=1 finetune_name=reg train_size_get=100 eval_size_get=40000 zero_shot_only=1  batch_size=100  n_steps_per_epoch=1  max_epoch=100 log_periodic=1 data.train_types=$type data.eval_types=$type model.name=prose meta=0

done


for type in "${pre_train_types[@]}"
do
  echo "Finetuning $type"
  pre_train=pretrain_reversefull_size1000
  NUMEXPR_MAX_THREADS=12 CUDA_VISIBLE_DEVICES=$GPU python src/main.py batch_size_eval=512 exp_name=fftune exp_id=lora20_TL_1000_${type} wandb.id=lora20_TL_1000_${type} save_periodic=-1 reload_model=checkpoint/${user}/dumped/LeMON_PROSE/${pre_train}/checkpoint.pth finetune=1 finetune_name=lora lora_r=64 lora_alpha=5 train_size_get=20 eval_size_get=40000 zero_shot_only=1  batch_size=20  n_steps_per_epoch=1  max_epoch=100 log_periodic=1 data.train_types=$type data.eval_types=$type model.name=prose meta=0
  NUMEXPR_MAX_THREADS=12 CUDA_VISIBLE_DEVICES=$GPU python src/main.py batch_size_eval=512 exp_name=fftune exp_id=lora50_TL_1000_${type} wandb.id=lora50_TL_1000_${type} save_periodic=-1 reload_model=checkpoint/${user}/dumped/LeMON_PROSE/${pre_train}/checkpoint.pth finetune=1 finetune_name=lora lora_r=64 lora_alpha=5 train_size_get=50 eval_size_get=40000 zero_shot_only=1  batch_size=50  n_steps_per_epoch=1  max_epoch=100 log_periodic=1 data.train_types=$type data.eval_types=$type model.name=prose meta=0
  NUMEXPR_MAX_THREADS=12 CUDA_VISIBLE_DEVICES=$GPU python src/main.py batch_size_eval=512 exp_name=fftune exp_id=lora100_TL_1000_${type} wandb.id=lora100_TL_1000_${type} save_periodic=-1 reload_model=checkpoint/${user}/dumped/LeMON_PROSE/${pre_train}/checkpoint.pth finetune=1 finetune_name=lora lora_r=64 lora_alpha=5 train_size_get=100 eval_size_get=40000 zero_shot_only=1  batch_size=100  n_steps_per_epoch=1  max_epoch=100 log_periodic=1 data.train_types=$type data.eval_types=$type model.name=prose meta=0

  NUMEXPR_MAX_THREADS=12 CUDA_VISIBLE_DEVICES=$GPU python src/main.py batch_size_eval=512 exp_name=fftune exp_id=ft20_TL_1000_${type} wandb.id=ft20_TL_1000_${type} save_periodic=-1 reload_model=checkpoint/${user}/dumped/LeMON_PROSE/${pre_train}/checkpoint.pth finetune=1 finetune_name=reg train_size_get=20 eval_size_get=40000 zero_shot_only=1  batch_size=20  n_steps_per_epoch=1  max_epoch=100 log_periodic=1 data.train_types=$type data.eval_types=$type model.name=prose meta=0
  NUMEXPR_MAX_THREADS=12 CUDA_VISIBLE_DEVICES=$GPU python src/main.py batch_size_eval=512 exp_name=fftune exp_id=ft50_TL_1000_${type} wandb.id=ft50_TL_1000_${type} save_periodic=-1 reload_model=checkpoint/${user}/dumped/LeMON_PROSE/${pre_train}/checkpoint.pth finetune=1 finetune_name=reg train_size_get=50 eval_size_get=40000 zero_shot_only=1  batch_size=50  n_steps_per_epoch=1  max_epoch=100 log_periodic=1 data.train_types=$type data.eval_types=$type model.name=prose meta=0
  NUMEXPR_MAX_THREADS=12 CUDA_VISIBLE_DEVICES=$GPU python src/main.py batch_size_eval=512 exp_name=fftune exp_id=ft100_TL_1000_${type} wandb.id=ft100_TL_1000_${type} save_periodic=-1 reload_model=checkpoint/${user}/dumped/LeMON_PROSE/${pre_train}/checkpoint.pth finetune=1 finetune_name=reg train_size_get=100 eval_size_get=40000 zero_shot_only=1  batch_size=100  n_steps_per_epoch=1  max_epoch=100 log_periodic=1 data.train_types=$type data.eval_types=$type model.name=prose meta=0


  pre_train=5stepmaml_reversefull_size1000
  NUMEXPR_MAX_THREADS=12 CUDA_VISIBLE_DEVICES=$GPU python src/main.py batch_size_eval=512 exp_name=fftune exp_id=lora20_MAML_1000_${type} wandb.id=lora20_MAML_1000_${type} save_periodic=-1 reload_model=checkpoint/${user}/dumped/LeMON_PROSE/${pre_train}/checkpoint.pth finetune=1 finetune_name=lora lora_r=64 lora_alpha=5 train_size_get=20 eval_size_get=40000 zero_shot_only=1  batch_size=20  n_steps_per_epoch=1  max_epoch=100 log_periodic=1 data.train_types=$type data.eval_types=$type model.name=prose meta=0
  NUMEXPR_MAX_THREADS=12 CUDA_VISIBLE_DEVICES=$GPU python src/main.py batch_size_eval=512 exp_name=fftune exp_id=lora50_MAML_1000_${type} wandb.id=lora50_MAML_1000_${type} save_periodic=-1 reload_model=checkpoint/${user}/dumped/LeMON_PROSE/${pre_train}/checkpoint.pth finetune=1 finetune_name=lora lora_r=64 lora_alpha=5 train_size_get=50 eval_size_get=40000 zero_shot_only=1  batch_size=50  n_steps_per_epoch=1  max_epoch=100 log_periodic=1 data.train_types=$type data.eval_types=$type model.name=prose meta=0
  NUMEXPR_MAX_THREADS=12 CUDA_VISIBLE_DEVICES=$GPU python src/main.py batch_size_eval=512 exp_name=fftune exp_id=lora100_MAML_1000_${type} wandb.id=lora100_MAML_1000_${type} save_periodic=-1 reload_model=checkpoint/${user}/dumped/LeMON_PROSE/${pre_train}/checkpoint.pth finetune=1 finetune_name=lora lora_r=64 lora_alpha=5 train_size_get=100 eval_size_get=40000 zero_shot_only=1  batch_size=100  n_steps_per_epoch=1  max_epoch=100 log_periodic=1 data.train_types=$type data.eval_types=$type model.name=prose meta=0

  NUMEXPR_MAX_THREADS=12 CUDA_VISIBLE_DEVICES=$GPU python src/main.py batch_size_eval=512 exp_name=fftune exp_id=ft20_MAML_1000_${type} wandb.id=ft20_MAML_1000_${type} save_periodic=-1 reload_model=checkpoint/${user}/dumped/LeMON_PROSE/${pre_train}/checkpoint.pth finetune=1 finetune_name=reg train_size_get=20 eval_size_get=40000 zero_shot_only=1  batch_size=20  n_steps_per_epoch=1  max_epoch=100 log_periodic=1 data.train_types=$type data.eval_types=$type model.name=prose meta=0
  NUMEXPR_MAX_THREADS=12 CUDA_VISIBLE_DEVICES=$GPU python src/main.py batch_size_eval=512 exp_name=fftune exp_id=ft50_MAML_1000_${type} wandb.id=ft50_MAML_1000_${type} save_periodic=-1 reload_model=checkpoint/${user}/dumped/LeMON_PROSE/${pre_train}/checkpoint.pth finetune=1 finetune_name=reg train_size_get=50 eval_size_get=40000 zero_shot_only=1  batch_size=50  n_steps_per_epoch=1  max_epoch=100 log_periodic=1 data.train_types=$type data.eval_types=$type model.name=prose meta=0
  NUMEXPR_MAX_THREADS=12 CUDA_VISIBLE_DEVICES=$GPU python src/main.py batch_size_eval=512 exp_name=fftune exp_id=ft100_MAML_1000_${type} wandb.id=ft100_MAML_1000_${type} save_periodic=-1 reload_model=checkpoint/${user}/dumped/LeMON_PROSE/${pre_train}/checkpoint.pth finetune=1 finetune_name=reg train_size_get=100 eval_size_get=40000 zero_shot_only=1  batch_size=100  n_steps_per_epoch=1  max_epoch=100 log_periodic=1 data.train_types=$type data.eval_types=$type model.name=prose meta=0



done